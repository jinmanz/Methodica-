{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import text\n",
    "import nltk \n",
    "\n",
    "f = open('Emma_Austen.txt','r')\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "- Next, we will first split the text into sentences using a sentence segmenter \"nltk.sent_tokenize\"\n",
    "- Each sentence will be further sibdivided into words using a word tokenizer \"nltk.word_tokenize\"\n",
    "- Next, each sentence will be tagged with part-of-speech tags using nltk.pos_tag, which will prove very helppful in the next step, name entity detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "- Now, we have done the preprocessing of the raw text. Next, we will search for mentions of potentially entities in each sentence. The basic technique we will use for entity detection is chunking.Chunking aims at grouping elements of the sequence, without any differentiation between obtained groups. For example, noun phrase chunking or verb group chunking. \n",
    "- We will begin by considering the task of noun phrase chunking, or NP-chunking, where we search for chunks corresponding to individual noun phrases. We will use part-of-speech tags help us for the NP chunking.\n",
    "    - In order to create an NP-chunker, we will first define a chunk grammar, consisting of rules that indicate how sentences should be chunked. \n",
    "    - In this case, we will define a simple grammar with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). \n",
    "    - Using this grammar, we create a chunk parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTKâ€™s currently recommended named entity chunker to chunk the given list of tagged sentences, \n",
    "# each consisting of a list of tagged tokens.\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to extract entity names in a chunked sentence\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label: # if the tree(t) has an attribute called \"label\" and its value is not none. \n",
    "        if t.label() == 'NE': # if the label is \"NE\", add this node to entity_names\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emma Woodhouse', 'Miss Taylor', 'Mr. Woodhouse', 'Emma', 'Miss Taylor', 'Miss Taylor']\n",
      "{'Miss Taylor', 'Emma Woodhouse', 'Mr. Woodhouse', 'Emma'}\n"
     ]
    }
   ],
   "source": [
    "# create a list to store the entity names\n",
    "entity_names = []\n",
    "\n",
    "# iterate the chunked sentences to extract entity names\n",
    "for tree in chunked_sentences:\n",
    "    entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "# Print all entity names\n",
    "print (entity_names)\n",
    "\n",
    "# Print unique entity names\n",
    "print (set(entity_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
