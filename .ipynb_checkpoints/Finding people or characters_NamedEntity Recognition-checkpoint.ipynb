{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import nltk \n",
    "\n",
    "text = \"Sixteen years had Miss Taylor been in Mr. Woodhouse's family, less as a governess than a friend, very fond of both daughters,but particularly of Emma.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "- Next, we will first split the text into sentences using a sentence segmenter \"nltk.sent_tokenize\"\n",
    "- Each sentence will be further sibdivided into words using a word tokenizer \"nltk.word_tokenize\"\n",
    "- Next, each sentence will be tagged with part-of-speech tags using nltk.pos_tag, which will prove very helppful in the next step, name entity detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "- Now, we have done the preprocessing of the raw text. Next, we will search for mentions of potentially entities in each sentence. The basic technique we will use for entity detection is chunking.Chunking aims at grouping elements of the sequence, without any differentiation between obtained groups. For example, noun phrase chunking or verb group chunking. \n",
    "- We will begin by considering the task of noun phrase chunking, or NP-chunking, where we search for chunks corresponding to individual noun phrases. We will use part-of-speech tags help us for the NP chunking.\n",
    "    - In order to create an NP-chunker, we will first define a chunk grammar, consisting of rules that indicate how sentences should be chunked. \n",
    "    - In this case, we will define a simple grammar with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). \n",
    "    - Using this grammar, we create a chunk parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for chunk in chunked_sentences:\\n    print chunk\\n    '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NLTKâ€™s currently recommended named entity chunker to chunk the given list of tagged sentences, \n",
    "# each consisting of a list of tagged tokens.\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "# each chunked_sentences is a tree with children tagged/labeled words.\n",
    "# those potential named entities are labeled by \"NE\" \n",
    "'''for chunk in chunked_sentences:\n",
    "    print chunk\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label: # if the tree(t) has an attribute called \"label\" and its value is not none. \n",
    "        if t.label() == 'NE': # if the label is \"NE\", add this node to entity_names\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "\n",
    "        else: # else, iterate the children nodes of the current node. \n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names'''\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['Emma', 'Mr. Woodhouse', 'Miss Taylor'])\n"
     ]
    }
   ],
   "source": [
    "'''entityNames = []\n",
    "for tree in chunked_sentences:\n",
    "    # extract named entities in each sentence(tree)\n",
    "    entityNames.extend(extract_entity_names(tree))\n",
    "    \n",
    "#print entity_names\n",
    "print entityNames\n",
    "# Print unique entity names\n",
    "print set(entityNames)'''\n",
    "entity_names = []\n",
    "for tree in chunked_sentences:\n",
    "    # Print results per sentence\n",
    "    # print extract_entity_names(tree)\n",
    "\n",
    "    entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "# Print all entity names\n",
    "#print entity_names\n",
    "\n",
    "# Print unique entity names\n",
    "print set(entity_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-71-e2c0dad7a795>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-71-e2c0dad7a795>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://gist.github.com/onyxfish/322906\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://gist.github.com/onyxfish/322906\n",
    "    https://gist.github.com/gornostal/1f123aaf838506038710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
